-----------------------------------------------------------
FASE 1. Estructura base del proyecto y prueba de ejecucion
-----------------------------------------------------------
OBJETIVO

Crear la arquitectura minima para un MLP con capas abstractas, sin implementacion aun.
Esta fase verifica que:
    - El proyecto compile correctamente.
    - Las clases se instancian y se recorren.
    - El flujo forward() y backward() funciona con capas dummy.

ESTRUCTURA DE LAS CARPETAS

MLP_base/
├── include/
│   ├── layer.h
│   ├── mlp.h
│   └── profiler.h
├── src/
│   ├── mlp.cpp
│   └── profiler.cpp
├── main.cpp
├── Makefile
└── README

-----------------------------------------------------------
FASE 2. Implementacion de la clase Linear como subclase de Layer
-----------------------------------------------------------
OBJETIVO

Implementar una capa Linear que herede de Layer, con:
    - Metodos forward_cpu, backward_cpu
    - Metodos forward_gpu, backward_gpu
    - Inicializacion de pesos y sesgos
    - Documentacion detallada por clase, metodo y variable
    - Prueba de ejecucion y verificacion funcional

-----------------------------------------------------------
FASE 3. Inicializacion Estandarizada de Pesos y Sesgos
-----------------------------------------------------------
OBJETIVO

Implemenar metodos de inicializacion de parametros en la clase Lienar,
siguiendo practicas comunes en Deep Learning para mejorar la estabilidad
del entrenamiento. Se mantiene la co

FUNDAMENTO TEORICO

La inicializacion adecuada de pesos evita problemas como:
    - Exploding Gradients: Cuando los valores crecen exponencialmente
    - Vanishing Gradients: Cuando los valores se reducen a cero

Metodo Recomendados:
    - Xavier (Glorot): Activaciones lineales o tanh
    - He (Kaiming): Activaciones ReLU

-----------------------------------------------------------
FASE 4. Implementacion de la Clase Activation como
        subclase de Layer. Se agrega soporte con cuBLAS
        y cuDNN
-----------------------------------------------------------
OBJETIVO

Disenar e implementar una capa de activacion que herede de Layer,
como soporte para funciones comunes como ReLU, Sigmoid y Tanh.
Esta capa debe ser completamente funcional en CPU y GPU, y debe
integrarse al flujo forward() y backward del MLP.

FUNDAMENTO TEORICO

Las funciones de activacion introducen no linealidad en la red,
permitiendo que el MLP aprenda representacione complejas. Cada
funcion tiene propiedades distintas:

    - ReLU: Capas ocultas, redes profundas
    - Sigmoid: Salidas binarias
    - Tanh: Salidas centradas en cero

CONSIDERACIONES TECNICAS

    - cuBLAS no implementa fuciones de activacion directamente,
        pero puede usarse para operaciones auxiliares
        (e.g. escalado, suma)
    - cuDNN si ofrece activaciones optimizadas, pero requiere
        estructuras especificas (cudnnTensorDescriptor_t,
        cudnnActivationDescriptor_t)
    - Para esta fase, se implementa ReLU con cuDNN y se deja
        Sigmoid/Tanh como extensiones

ESTRUCTURA PROPUESTA

MLP_base/
├── include/
│   ├── activation.h      // Declaración de clase Activation
├── src/
│   ├── activation.cpp    // Implementación de métodos

