-----------------------------------------------------------
FASE 1. Estructura base del proyecto y prueba de ejecucion
-----------------------------------------------------------
OBJETIVO

Crear la arquitectura minima para un MLP con capas abstractas, sin implementacion aun.
Esta fase verifica que:
    - El proyecto compile correctamente.
    - Las clases se instancian y se recorren.
    - El flujo forward() y backward() funciona con capas dummy.

ESTRUCTURA DE LAS CARPETAS

MLP_base/
├── include/
│   ├── layer.hpp
│   ├── mlp.hpp
│   └── profiler.hpp
├── src/
│   ├── mlp.cpp
│   └── profiler.cpp
├── main.cpp
├── Makefile
└── README

-----------------------------------------------------------
FASE 2. Implementacion de la clase Linear como subclase de Layer
-----------------------------------------------------------
OBJETIVO

Implementar una capa Linear que herede de Layer, con:
    - Metodos forward_cpu, backward_cpu
    - Metodos forward_gpu, backward_gpu
    - Inicializacion de pesos y sesgos
    - Documentacion detallada por clase, metodo y variable
    - Prueba de ejecucion y verificacion funcional

-----------------------------------------------------------
FASE 3. Inicializacion Estandarizada de Pesos y Sesgos
-----------------------------------------------------------
OBJETIVO

Implemenar metodos de inicializacion de parametros en la clase Lienar,
siguiendo practicas comunes en Deep Learning para mejorar la estabilidad
del entrenamiento. Se mantiene la co

FUNDAMENTO TEORICO

La inicializacion adecuada de pesos evita problemas como:
    - Exploding Gradients: Cuando los valores crecen exponencialmente
    - Vanishing Gradients: Cuando los valores se reducen a cero

Metodo Recomendados:
    - Xavier (Glorot): Activaciones lineales o tanh
    - He (Kaiming): Activaciones ReLU

-----------------------------------------------------------
FASE 4. Implementacion de la Clase Activation como
        subclase de Layer. Se agrega soporte con cuBLAS
        y cuDNN
-----------------------------------------------------------
OBJETIVO

Disenar e implementar una capa de activacion que herede de Layer,
como soporte para funciones comunes como ReLU, Sigmoid y Tanh.
Esta capa debe ser completamente funcional en CPU y GPU, y debe
integrarse al flujo forward() y backward del MLP.

FUNDAMENTO TEORICO

Las funciones de activacion introducen no linealidad en la red,
permitiendo que el MLP aprenda representacione complejas. Cada
funcion tiene propiedades distintas:

    - ReLU: Capas ocultas, redes profundas
    - Sigmoid: Salidas binarias
    - Tanh: Salidas centradas en cero

CONSIDERACIONES TECNICAS

    - cuBLAS no implementa fuciones de activacion directamente,
        pero puede usarse para operaciones auxiliares
        (e.g. escalado, suma)
    - cuDNN si ofrece activaciones optimizadas, pero requiere
        estructuras especificas (cudnnTensorDescriptor_t,
        cudnnActivationDescriptor_t)
    - Para esta fase, se implementa ReLU con cuDNN y se deja
        Sigmoid/Tanh como extensiones

ESTRUCTURA PROPUESTA

MLP_base/
├── include/
│   ├── activation.hpp      // Declaración de clase Activation
├── src/
│   ├── activation.cpp    // Implementación de métodos

-----------------------------------------------------------
FASE 5. Encapsulamiento de la logica GPU para activaciones
-----------------------------------------------------------
OBJETIVO

Separar la logica de ejecucion en GPU (cuDNN) de la clase Activation, creando
una clase auciliar que gestione:
    - La asignacion y liberacion de memoria en GPU
    - Las llamadas a cudnnActivationForward y cudnnActivarionBackward
    - La transferencia de datos entre CPU y GPU
    - La trazabilidad y validacion de error CUDA/cuDNN.

esto permite que la clase Activation se mantenga limpia y enfocada en la
semantica de capa, mientras que la clase auxiliar se encarga de los
detalles de ejecucion en GPU.

FUNDAMENTO TEORICO

Encapsular la logica GPU permite aplicar principios de diseno como:

    - Responsabilidad unica: Activation define la capa; GpuActivationHelper ejecuta en GPU
    - Modularidad: Se puede probar, extender o reemplazar la logica GPU sin tocar la capa.
    - Reusabilidad: El helper puede usarse en otras capas con activaciones similares.
    - Validacion incremental: Permite comparar CPU vs GPU de forma controlada.

ESTRUCTURA PROPUESTA

MLP_base/
├── include/
│   └── gpu_activation_helper.hpp
├── src/
│   └── gpu_activation_helper.cpp


NOTA:
    Se creo un archivo gpu_error_checking.hpp para los macros
        - CUDA_CHECK
        - CUDNN_CHECK


-----------------------------------------------------------
FASE 6. Test unitarios y registros de tiempo
-----------------------------------------------------------
FASE 6A. Tests unitarios para validacion funcional
         y consistencia numerica
-----------------------------------------------------------
OBJETIVOS
    - Verificar que forward_cpu ≈ forward_gpu y backward_cpu ≈ backward_gpu
    - Detectar errores de calculo, desbordamiento o inconsistencias
    - Establecer tolerancias numericas razonables (por ejemplo, 1e-5)

-----------------------------------------------------------
FASE 6B. Registro de tiempos de ejecucion
-----------------------------------------------------------
OBJETIVOS

    - Medir tiempo de ejecucion por capa (forward, backward)
    - Medir tiempo total del modelo
    - Comparar CPU vs GPU

-----------------------------------------------------------
FASE 7. Entrenamiento supervisado - estructura base
-----------------------------------------------------------
OBJETIVOS
    - Implmentar clases modulares para funcion perdida, optimizador y entrenamiento.
    - Soporte regresion (MSE) y clasificacion (softmax + CrossEntropy).
    - Mantener la compatibilidad con CPU y GPU
    - Preparar el ciclo forward → loss → backward → update

ESTRUCTURA SUGERIDA

MLP_base/
├── include/
│   ├── loss.hpp
│   ├── optimizer.hpp
│   └── trainer.hpp
├── src/
│   ├── loss.cpp
│   ├── optimizer.cpp
│   └── trainer.cpp



